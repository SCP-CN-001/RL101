{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 时序差分与Q-Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **时序差分 (TD)**\n",
    "\n",
    "时序差分方法即在环境参数未知时以采样的方式更新价值函数。在 $t$ 时刻，智能体与未知环境交互转移到下一个状态 $S_{t+1}$ 并获得奖励 $R_{t+1}$，定义差分误差\n",
    "\n",
    "$$\n",
    "\\delta_{t} \\doteq R_{t+1}+\\gamma V\\left(S_{t+1}\\right)-V\\left(S_{t}\\right)\n",
    "$$\n",
    "\n",
    "由价值函数的定义知 $R_{t+1}+\\gamma V\\left(S_{t+1}\\right)$ 实际上是对 $V(S_t)$ 的采样。把采样值和当前值做差得到差分误差，并采用如下方式更新 $V_(S_t)$\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "V\\left(S_{t}\\right) & \\leftarrow V\\left(S_{t}\\right)+\\alpha\\delta_{t} \\\\\n",
    "& = V\\left(S_{t}\\right)+\\alpha\\left[R_{t+1}+\\gamma V\\left(S_{t+1}\\right)-V\\left(S_{t}\\right)\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中 $\\alpha$ 为学习率，$\\lambda$ 为折扣因子。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q-learning**\n",
    "\n",
    "Q-learning 即采取如下时序差分方式对 Q 值进行更新，迭代地求解有限离散状态下的 MDP 问题。\n",
    "\n",
    "$$\n",
    "Q\\left(S_{t}, A_{t}\\right) \\leftarrow Q\\left(S_{t}, A_{t}\\right)+\\alpha\\left[R_{t+1}+\\gamma \\max _{a} Q\\left(S_{t+1}, a\\right)-Q\\left(S_{t}, A_{t}\\right)\\right]\n",
    "$$\n",
    "\n",
    "其中 $\\alpha$ 为学习率，$\\lambda$ 为折扣因子。Q-learning 迭代地遍历每一个状态-动作对，可以证明：对于有限离散的 MDP 问题，Q-learning 会使所有的状态-动作对对应的 Q 值收敛到最优值，从而给出最优值函数和最优策略。\n",
    "\n",
    "由上述迭代式，可以给出一个简易的 Q-learning 的类实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class QLearning:\n",
    "    def __init__(self, num_states, num_actions, learning_rate, discount_factor):\n",
    "        self.num_states = num_states  # 总状态数\n",
    "        self.num_actions = num_actions  # 总动作数\n",
    "        self.lr = learning_rate  # 学习率\n",
    "        self.discount = discount_factor  # 折扣因子\n",
    "\n",
    "        self.q_table = np.zeros((num_states, num_actions))\n",
    "        self.epsilon = 0.1\n",
    "\n",
    "    # 依epsilon-greedy策略选择动作\n",
    "    def choose_action(self, state):\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            action = np.random.randint(self.num_actions)\n",
    "        else:\n",
    "            action = np.argmax(self.q_table[state])\n",
    "        return action\n",
    "\n",
    "    # 使用Q-learning更新Q值表\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        q_value = self.q_table[state, action]\n",
    "        max_q_value = np.max(self.q_table[next_state])\n",
    "        td_error = reward + self.discount * max_q_value - q_value\n",
    "        self.q_table[state, action] += self.lr * td_error\n",
    "\n",
    "    def train(self, num_episodes, env):\n",
    "        for episode in range(num_episodes):\n",
    "            state = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.choose_action(state)\n",
    "                # 执行动作，获得环境返回的奖励和下一个状态\n",
    "                next_state, reward, done = env.step(state, action)\n",
    "                self.update_q_table(state, action, reward, next_state)\n",
    "                state = next_state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用一个简单的确定性环境测试代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Q-table:\n",
      "[[ 5.471       5.3780294 ]\n",
      " [ 6.94792973  7.19      ]\n",
      " [ 0.          0.        ]\n",
      " [ 9.1         8.83673254]\n",
      " [-1.         -1.        ]\n",
      " [ 0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.num_states = 6\n",
    "        self.num_actions = 2\n",
    "\n",
    "    def step(self, state, action):\n",
    "        transitions = [[0, 1, -1], [1, 3, -1], [2, 3, -1], [3, 4, 10], [4, 5, -1], [5, 5, 0]]\n",
    "        next_state, reward, done = transitions[state][1], transitions[state][2], False\n",
    "        if next_state == 5:\n",
    "            done = True\n",
    "        return next_state, reward, done\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = Environment()\n",
    "    q_learning = QLearning(env.num_states, env.num_actions, learning_rate = 0.5, discount_factor = 0.9)\n",
    "    q_learning.train(num_episodes = 100, env = env)\n",
    "\n",
    "    print(\"Learned Q-table:\")\n",
    "    print(q_learning.q_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
