{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 时序差分与Q-Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **时序差分 (TD)**\n",
    "\n",
    "时序差分方法即在环境参数未知时以采样的方式更新价值函数。在 $t$ 时刻，智能体与未知环境交互转移到下一个状态 $S_{t+1}$ 并获得奖励 $R_{t+1}$，定义差分误差\n",
    "\n",
    "$$\n",
    "\\delta_{t} \\doteq R_{t+1}+\\gamma V\\left(S_{t+1}\\right)-V\\left(S_{t}\\right)\n",
    "$$\n",
    "\n",
    "由价值函数的定义知 $R_{t+1}+\\gamma V\\left(S_{t+1}\\right)$ 实际上是对 $V(S_t)$ 的采样。把采样值和当前值做差得到差分误差，并采用如下方式更新 $V_(S_t)$\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "V\\left(S_{t}\\right) & \\leftarrow V\\left(S_{t}\\right)+\\alpha\\delta_{t} \\\\\n",
    "& = V\\left(S_{t}\\right)+\\alpha\\left[R_{t+1}+\\gamma V\\left(S_{t+1}\\right)-V\\left(S_{t}\\right)\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中 $\\alpha$ 为学习率，$\\lambda$ 为折扣因子。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q-learning**\n",
    "\n",
    "Q-learning 即采取如下时序差分方式对 Q 值进行更新，迭代地求解有限离散状态下的 MDP 问题。\n",
    "\n",
    "$$\n",
    "Q\\left(S_{t}, A_{t}\\right) \\leftarrow Q\\left(S_{t}, A_{t}\\right)+\\alpha\\left[R_{t+1}+\\gamma \\max _{a} Q\\left(S_{t+1}, a\\right)-Q\\left(S_{t}, A_{t}\\right)\\right]\n",
    "$$\n",
    "\n",
    "其中 $\\alpha$ 为学习率，$\\lambda$ 为折扣因子。Q-learning 迭代地遍历每一个状态-动作对，可以证明：对于有限离散的 MDP 问题，Q-learning 会使所有的状态-动作对对应的 Q 值收敛到最优值，从而给出最优值函数和最优策略。\n",
    "\n",
    "由上述迭代式，可以给出一个简易的 Q-learning 的类实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, learning_rate=0.1, discount_factor=0.9, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.state_dim = env.observation_space.shape\n",
    "        self.action_dim = env.action_space.n\n",
    "\n",
    "        self.Qtable = defaultdict(lambda: np.zeros(self.action_dim))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(self.Qtable[state])\n",
    "        return action\n",
    "\n",
    "    def update(self, state, action, reward, done, next_state):\n",
    "        q_value = self.Qtable[state][action]\n",
    "        max_q_value = np.max(self.Qtable[next_state])\n",
    "        td_target = reward + self.discount_factor * max_q_value * (1 - done)\n",
    "        td_error = td_target - q_value\n",
    "        self.Qtable[state][action] += self.learning_rate * td_error\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                self.update(state, action, reward, terminated, next_state)\n",
    "                done = terminated or truncated\n",
    "                state = next_state\n",
    "\n",
    "    def save(self, file_path):\n",
    "        checkpoint = {\n",
    "            'Qtable': dict(self.Qtable),\n",
    "            'state_dim': self.state_dim,\n",
    "            'action_dim': self.action_dim\n",
    "        }\n",
    "        torch.save(checkpoint, file_path)\n",
    "\n",
    "    def load(self, file_path):\n",
    "        checkpoint = torch.load(file_path)\n",
    "        self.Q = defaultdict(lambda: np.zeros(self.action_dim), checkpoint['Q'])\n",
    "        self.state_dim = checkpoint['state_dim']\n",
    "        self.action_dim = checkpoint['action_dim']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用 gym 里的 blackjack 游戏验证并保存 checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make('Blackjack-v1')\n",
    "agent = QLearningAgent(env)\n",
    "agent.train(num_episodes=1000)\n",
    "\n",
    "save_path = './blackjack_ckpt.pt'\n",
    "agent.save(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
