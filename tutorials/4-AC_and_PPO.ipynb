{"cells":[{"cell_type":"markdown","metadata":{"id":"7C0480D97F344544BC2824225ADBF12D","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["# 第四次作业：Actor-Critic 和 PPO 算法\n","\n","**本次作业一共包括两个部分：ActorCritic算法和PPO算法。我们分别需要分别实现两个算法，并在离散和连续动作环境（CartPole和Pendulum）下测试算法效果。**\n","\n","**同学们需要完成三个代码实现（通过注释方式标出），并调节参数画出实验效果图。**\n","\n","# 第一部分：ActorCritic算法\n","## 简介\n","之前我们介绍的Q-learning和DQN及改进算法都是基于值函数（value-based）的方法，其中Q-learning是处理有限状态的算法，而DQN可以用来解决连续状态的问题。在强化学习中，除了基于值函数的方法，还有一支非常经典的方法，那就是基于策略（policy-based）的方法。对比两者，基于值函数的方法主要是学习值函数，然后根据值函数导出一个策略，此时并不存在一个显式的策略。而基于策略的方法则是直接显式的学习一个目标策略。策略梯度是基于策略的方法的基础，我们将从策略梯度算法说起。\n","## 策略梯度\n","基于策略的方法首先需要参数化策略，我们假设目标策略$\\pi_\\theta$是一个随机性策略，并且处处可微，其中$\\theta$是对应的参数。我们的目标是要寻找一个最优策略，来最大化这个策略在环境中的期望回报。我们将策略学习的目标函数定义为\n","$$\n","J(\\theta)= V^{\\pi_\\theta}(s_0)\n","$$\n","其中$s_0$表示初始状态。现在有了目标函数，我们将目标函数对策略$\\theta$求导，得到导数后，我们就可以用梯度上升方法来最大化这个目标函数从而得到最优策略。\n","\n","我们之前在MDP章节中学习过在策略$\\pi$下的状态访问分布，我们在此用$d^{\\pi}$表示。然后我们对目标函数求梯度，可以得到如下式子，更详细的推导将在扩展阅读中给出。\n","\n","$$\n","\\begin{align}\n","\\nabla_{\\theta}J(\\theta)\n","&\\propto \\sum_{s \\in S}d^{\\pi}(s)\\sum_{a \\in A}Q^{\\pi}(s,a)\\nabla_{\\theta}\\pi_{\\theta}(a|s)\\\\\n","&=\\sum_{s \\in S}d^{\\pi}(s)\\sum_{a \\in A}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)\\frac{\\nabla_{\\theta}\\pi_{\\theta}(a|s)}{\\pi_{\\theta}(a|s)}\\\\\n","&= \\mathbb{E}_{\\pi}[Q^{\\pi}(s,a)\\nabla_{\\theta}\\ln \\pi_{\\theta}(a|s)]\n","\\end{align}\n","$$\n","于是，我们就可以用这个梯度来更新策略。需要注意的是，因为上式期望的下标是$\\pi$，所以策略梯度算法为在线学习算法，即必须使用策略$\\pi_\\theta$采样得到的数据来计算梯度。更一般地，我们可以把梯度写成下面这个形式：\n","$$\n","g = \\mathbb{E}[\\sum^{\\infty}_{t=0}\\psi_{t}\\nabla_{\\theta}\\log \\pi_{\\theta}(a_{t}|s_{t})]\n","$$\n","其中$\\psi_{t}$可以有很多种形式：\n","$$\n","\\begin{align}\n","&1.\\psi_{t}=\\sum_{t=0}^{\\infty}\\gamma^t r_{t} : 轨迹的总回报  &&4.Q^{\\pi}(s_{t},a_{t}) : 动作价值函数\\\\\n","&2.\\psi_{t}=\\sum_{t'=t}^{\\infty} \\gamma^{t'-t} r_{t'} : 动作 a_{t}之后的回报 &&5.A^{\\pi}(s_{t},a_{t}): 优势函数\\\\\n","&3.\\psi_{t}=\\sum_{t'=t}^{\\infty}r_{t'}-b(s_{t}) : 基准线版本的改进 \\quad\\quad \\quad\\quad &&6.r_{t} + V^{\\pi}(s_{t+1}) - V^{\\pi}(s_t) : 时序差分残差\n","\\end{align}\n","$$\n","至此，我们已经了解了策略梯度的目标与形式，那么我们今天要讲的ActorCritic算法的流程是怎样的呢？\n","## ActorCritic 算法\n","最基本的PolicyGradient算法就是上文中的1式$\\psi_{t}=\\sum_{t=0}^{\\infty}\\gamma^t r_{t}$，而ActorCritic就是通过value function来估计return值，即在上文中$\\psi=Q^{\\pi}(s_{t},a_{t})$式。\n","\n","两个常见的改进：baseline和advantage分别对应3和5式。结合两个改进，得到我们今天需要实现的AdvantageActorCritic算法，即式6: \n","$$\\psi_{t}=r_{t} + V^{\\pi}(s_{t+1}) - V^{\\pi}(s_t)$$\n","\n","具体流程如下：\n","$$\n","\\begin{align}\n","&1.初始化策略参数\\theta \\\\\n","&2.用当前策略\\pi_\\theta采样轨迹\\{s_{1},a_{1},r_{1},s_{2},a_{2},r_{2} ... s_{t},a_{t},r_{t}\\}\\\\\n","&3.计算当前轨迹的回报\\sum_{t'=t}^{\\infty}\\gamma^{t'-t}r_{t'}作为\\psi_{t}\\\\\n","&4.对\\theta进行更新 \\theta = \\theta + \\alpha \\psi_{t}\\nabla_{\\theta}\\log \\pi_{\\theta}(a_{t}|s_{t})\\\\\n","&5.重复2-4步\n","\\end{align}\n","$$\n","\n","好了！这就是REINFORCE算法的全部啦，让我们来用代码实现它看看效果如何吧！\n","\n"]},{"cell_type":"markdown","metadata":{"id":"880649899BDF4E868E82960D05260E37","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["## 代码实践\n","我们在Cartpole环境中进行REINFORCE算法的实验。首先我们导入包"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2F59DF8812534F36874771F8720C93B7","jupyter":{},"notebookId":"5f8f327c46ba5e00307827a4","slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":["!pip install gym\n","import gym"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"40B616B26E8B4AA48E98B4258C7988CA","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","from torch import nn\n","from torch.distributions import Categorical, Normal\n","import sys\n","import math\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","\n","if torch.cuda.is_available():\n","    device = \"cuda:0\"\n","else:\n","    device = \"cpu\"\n","    \n","eps = np.finfo(np.float32).eps.item()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def smooth_curve(y, smooth):\n","    r = smooth\n","    length = int(np.prod(y.shape))\n","    for i in range(length):\n","        if i > 0:\n","            if (not np.isinf(y[i - 1])) and (not np.isnan(y[i - 1])):\n","                y[i] = y[i - 1] * r + y[i] * (1 - r)\n","    return y\n","\n","def moving_average(y, x=None, total_steps=100, smooth=0.9, move_max=False):\n","    if isinstance(y, list):\n","        y = np.array(y)\n","    length = int(np.prod(y.shape))\n","    if x is None:\n","        x = list(range(1, length+1))\n","    if isinstance(x, list):\n","        x = np.array(x)\n","    if length > total_steps:\n","        block_size = length//total_steps\n","        select_list = list(range(0, length, block_size))\n","        select_list = select_list[:-1]\n","        y = y[:len(select_list) * block_size].reshape(-1, block_size)\n","        if move_max:\n","            y = np.max(y, -1)\n","        else:\n","            y = np.mean(y, -1)\n","        x = x[select_list]\n","    y = smooth_curve(y, smooth)\n","    return y, x\n","def plotReward(infos):\n","    x, y = infos[\"episodes\"],infos[\"rewards\"]\n","    y, x = moving_average(y, x)\n","    plt.xlabel(\"Episode\")\n","    plt.ylabel(\"Reward\")\n","    plt.plot(x, y)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"7C0E3A3A844549BC905ED109D604EF92","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["定义基本网络单元Net"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"864658B0470F4AB4823322580ED41CDA","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":["class Net(torch.nn.Module):\n","    def __init__(self, hidden_size, input_size, output_size, layer_num=2):\n","        super(Net, self).__init__()\n","        # Use torch.nn.ModuleList or torch.nn.Sequential For multiple layers\n","        layers = []\n","        last_size = input_size\n","        for i in range(layer_num-1):\n","            layers.append(torch.nn.Linear(last_size, hidden_size))\n","            layers.append(torch.nn.ReLU())\n","            last_size = hidden_size\n","        layers.append(torch.nn.Linear(last_size, output_size))\n","        self._net = torch.nn.Sequential(*layers)\n","\n","    def forward(self, inputs):\n","        return self._net(inputs)"]},{"cell_type":"markdown","metadata":{"id":"6A19E6CFCEDC467BA350D1200C7E28B6","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["再定义RL算法的基类RLAlgo，我们已经在该类中实现离散和连续动作的分布计算等函数"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def trans2tensor(batch):\n","    for k in batch:\n","        # print(batch[k])\n","        if isinstance(batch[k], torch.Tensor):\n","            batch[k] = batch[k].to(device=device)\n","        elif isinstance(batch[k][0], torch.Tensor):\n","            batch[k] = torch.cat(batch[k]).to(device=device)\n","        else:\n","            batch[k] = torch.tensor(batch[k], device=device, dtype=torch.float32)\n","\n","    return batch\n","class RLAlgo:\n","    def __init__(self,discrete_action=True, tau=0.01):\n","        self.discrete_action = discrete_action\n","        self._tau = tau\n","    \n","    # update target network\n","    def soft_update(self, source, target, tau=None):\n","        if tau is None:\n","            tau = self._tau\n","        with torch.no_grad():\n","            for target_param, param in zip(target.parameters(), source.parameters()):\n","                target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n","    \n","    # sample actions\n","    def action(self, state):\n","        with torch.no_grad():\n","            p_out = self.policy(state)\n","        log_prob, action = self.dist(p_out)\n","        return action, log_prob\n","    \n","    # calculate probability of action\n","    def dist(self, p_out, action=None):\n","        if self.discrete_action:\n","            # ========================================\n","            # 【代码实现 1】: 请根据以下要求完成代码\n","            #  1. 根据policy网络输出p_out，通过softmax计算动作的分布dist\n","            #  2. 若action为None，根据分布dist采样得到动作action；若action不为None，使用输入的action\n","            #  3. 根据具体动作action和分布dist获得每个动作的概率log_prob\n","            # ========================================\n","            pass\n","            # ========================================\n","            # 【代码实现 1】\n","            # ========================================\n","        else:\n","            # only for pendulum env\n","            mean, var = torch.chunk(p_out, 2, dim=-1)\n","            mean = 2*torch.tanh(mean)\n","            var = torch.nn.functional.softplus(var)\n","            m = Normal(mean, var)\n","            if action is None:\n","                action = m.sample()\n","            log_prob = m.log_prob(action)\n","        \n","        return log_prob.reshape(-1, 1), action.reshape(-1, 1)\n","    \n","    # compute advantage\n","    def compute_adv(self, batch, gamma):\n","        s = batch[\"state\"]\n","        a = batch[\"action\"]\n","        r = batch[\"reward\"].reshape(-1, 1)\n","        s1 = batch[\"next_state\"]\n","        done = batch[\"done\"].reshape(-1, 1)\n","        old_log_prob = batch[\"log_prob\"].reshape(-1, 1)\n","        with torch.no_grad():\n","            adv = r + gamma * (1-done) * self.value(s1) - self.value(s)\n","\n","        return adv"]},{"cell_type":"markdown","metadata":{},"source":["接下来，请实现ActorCritic算法。\n","\n","在更新Agent的过程中，我们按照算法，将损失函数写为$L=(r_{t} + V^{\\pi}(s_{t+1}) - V^{\\pi}(s_t)) \\log\\pi_{\\theta}(a_{t}|s_{t})$，对$\\theta$求导就可以更新策略。\n","\n","需要注意，分别计算value和policy网络的损失函数loss时，需要对无关变量进行detach，以免梯度计算错误。"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ========================================\n","# 【代码实现 2】: 补全完成ActorCritic算法\n","#  DEBUG建议： 1. 检查是否对无关变量detach 2.检查两个tenor运算前是否对齐维度 3.检查公式是否写错\n","# ========================================\n","class AdvantageActorCritic(RLAlgo):\n","    def __init__(\n","        self, \n","        hidden_size, state_space, action_space, \n","        learning_rate, **kwargs\n","    ):\n","        super(AdvantageActorCritic, self).__init__( **kwargs)\n","        self.action_space = action_space\n","\n","        input_size = state_space.shape[0]\n","        if self.discrete_action:\n","            output_size = action_space.n\n","        else:\n","            output_size = action_space.shape[0]*2\n","    \n","    def update(self, batch, gamma):\n","        s = batch[\"state\"]\n","        a = batch[\"action\"].reshape(-1, 1)\n","        r = batch[\"reward\"].reshape(-1, 1)\n","        s1 = batch[\"next_state\"]\n","        done = batch[\"done\"].reshape(-1, 1)\n","        \n","        pass"]},{"cell_type":"markdown","metadata":{"id":"F6DDA156DE31493CA22217FA65E3C43E","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["定义好策略，我们就可以开始实验了，看看在Cartpole环境上表现如何吧！"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["env_name = \"CartPole-v0\"\n","env = gym.make(env_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A9993AF344AE4770896E0504E77BAF08","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":["max_timesteps = 1000\n","gamma = 0.99\n","\n","env_name = \"CartPole-v0\"\n","env = gym.make(env_name)\n","\n","\n","def train(agent, num_episode=1000, mini_epoch=5, discrete_action=True, print_every=100):\n","    rewards_log = []\n","    episodes_log = []\n","    for i_episode in range(num_episode):\n","        states = []\n","        acts = []\n","        rewards = []\n","        next_states = []\n","        log_probs = []\n","        dones = []\n","        \n","        state = env.reset()\n","        env.seed(1)\n","        random.seed(1)\n","        \n","        state = torch.tensor([state], device=device, dtype=torch.float32)\n","        \n","        for time_step in range(max_timesteps):\n","            action, log_prob = agent.action(state)\n","            action = action.cpu()\n","            if discrete_action:\n","                next_state, reward, done, _ = env.step(action.numpy().reshape(-1)[0])\n","            else:\n","                \n","                next_state, reward, done, _ = env.step(action.numpy().reshape(-1))\n","                \n","            # collect samples\n","            states.append(state)\n","            acts.append(action)\n","            log_probs.append(log_prob.detach())\n","            rewards.append(reward)\n","            next_states.append(next_state)\n","            dones.append(done)\n","            # log_probs.append(log_prob)\n","            \n","            state = torch.tensor([next_state], device=device, dtype=torch.float32)\n","            if done:\n","                break\n","        rewards_log.append(np.sum(rewards))\n","        episodes_log.append(i_episode)\n","        batch = trans2tensor({\"state\": states, \"action\":acts, \n","                          \"log_prob\": log_probs,\n","                          \"next_state\": next_states, \"done\":dones, \n","                          \"reward\": rewards, })\n","        r = batch[\"reward\"] \n","        batch[\"reward\"] = (r - r.mean()) / (r.std() + 1e-8)\n","        batch[\"adv\"] = agent.compute_adv(batch, gamma)\n","        for i in range(mini_epoch):\n","            agent.update(batch, gamma)\n","        if (i_episode + 1) % print_every == 0 or i_episode + 1 == num_episode:\n","            print(\"Episode: {}, Reward: {}\".format(i_episode+1, np.mean(rewards_log[-10:])))\n","    # env.close()\n","    infos = {\n","        \"rewards\": rewards_log,\n","        \"episodes\": episodes_log\n","    }\n","    return infos"]},{"cell_type":"markdown","metadata":{"id":"CB975960A51D4D97A708F800DAEB03EE","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["在CartPole-v0环境中，满分就是200分，让我们来看看每个Episode得分如何吧！"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 请自行调节参数\n","learning_rate = \n","hidden_size= \n","agent = AdvantageActorCritic(\n","        hidden_size, \n","        env.observation_space, \n","        env.action_space, \n","        learning_rate,\n","    )\n","ac_cartpole_infos = train(agent, 2000, 10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D2B1AD392B66419791F4FD6CB800A4A9","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":["plt.title('Reinforce on {}'.format(env_name))  \n","plt.ylabel(\"Reward\")\n","plt.xlabel(\"Frame\")\n","infos = [ac_cartpole_infos,]\n","labels = [\"ac\", ]\n","for info in infos:\n","    x, y = info[\"episodes\"],info[\"rewards\"]\n","    y, x = moving_average(y, x)\n","    plt.plot(x, y)\n","plt.legend(labels)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"C7BF78B87D294A038E9BEA15C75E8940","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["## 拓展阅读：策略梯度证明\n","我们要证明$\\nabla_{\\theta}J(\\theta) \\propto \\sum_{s \\in S}d^{\\pi}(s)\\sum_{a \\in A}Q^{\\pi}(s,a)\\nabla_{\\theta}\\pi_{\\theta}(a|s)$\n","                        \n","先从状态价值函数的推导开始：\n","$$\n","\\begin{align}\n","\\nabla_{\\theta}V^{\\pi}(s) &=\\nabla_{\\theta}(\\sum_{a \\in A} \\pi_{\\theta}(a|s)Q^{\\pi}(s,a)) \\\\\n","&=\\sum_{a\\in A}(\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a) + \\pi_{\\theta}(a|s)\\nabla_{\\theta}Q^{\\pi}(s,a))\\\\\n","&=\\sum_{a\\in A}(\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a) + \\pi_{\\theta}(a|s)\\nabla_{\\theta}\\sum_{s',r}P(s',r|s,a)(r+V^{\\pi}(s'))\\\\\n","&=\\sum_{a\\in A}(\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a) + \\pi_{\\theta}(a|s)\\sum_{s',r}P(s',r|s,a)\\nabla_{\\theta}V^{\\pi}(s'))\\\\\n","&=\\sum_{a\\in A}(\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a) + \\pi_{\\theta}(a|s)\\sum_{s'}P(s'|s,a)\\nabla_{\\theta}V^{\\pi}(s'))\\\\\n","\\end{align}\n","$$\n","为了简化表示，我们让$\\phi(s)=\\sum_{a \\in A}\\nabla_{\\theta}\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)$, 定义$\\rho^{\\pi}(s\\rightarrow x, k)$为策略$\\pi$从状态s出发k步后到达状态x的概率，我们继续推导:\n","$$\n","\\begin{align}\n","\\nabla_{\\theta}V^{\\pi}(s) &= \\phi(s) + \\sum_{a}\\pi_{\\theta}(a|s)\\sum_{s'}P(s'|s,a)\\nabla_{\\theta}V^{\\pi}(s')\\\\\n","&= \\phi(s) + \\sum_{a}\\sum_{s'}\\pi_{\\theta}(a|s)P(s'|s,a)\\nabla_{\\theta}V^{\\pi}(s')\\\\\n","&= \\phi(s) + \\sum_{s'}\\rho^{\\pi}(s \\rightarrow s',1)\\nabla_{\\theta}V^{\\pi}(s')\\\\\n","&= \\phi(s) + \\sum_{s'}\\rho^{\\pi}(s \\rightarrow s',1)[\\phi(s') + \\sum_{s''}\\rho^{\\pi}(s' \\rightarrow s'',1)\\nabla_{\\theta}V^{\\pi}(s'')]\\\\\n","&= \\phi(s) + \\sum_{s'}\\rho^{\\pi}(s \\rightarrow s',1)\\phi(s') + \\sum_{s''}\\rho^{\\pi}(s \\rightarrow s'',2)\\nabla_{\\theta}V^{\\pi}(s'')\\\\\n","&= \\phi(s) + \\sum_{s'}\\rho^{\\pi}(s \\rightarrow s',1)\\phi(s') +\\sum_{s''}\\rho^{\\pi}(s' \\rightarrow s'',2)\\phi(s'') + \\sum_{s'''}\\rho^{\\pi}(s \\rightarrow s''',3)\\nabla_{\\theta}V^{\\pi}(s''')\\\\\n","&= ......\\\\\n","&= \\sum_{x \\in S}\\sum^{\\infty}_{k=0}\\rho^{\\pi}(s \\rightarrow x, k)\\phi(x)\n","\\end{align}\n","$$\n","OK! 我们定义$\\eta(s)= \\sum^{\\infty}_{k=0}\\rho^{\\pi}(s_{0} \\rightarrow s, k)$至此我们看回我们的目标函数：\n","$$\n","\\begin{align}\n","\\nabla_{\\theta}J(\\theta) &= \\nabla_{\\theta}V^{\\pi}(s_{0})\\\\\n","&= \\sum_{s}\\sum^{\\infty}_{k=0}\\rho^{\\pi}(s_{0} \\rightarrow s, k)\\phi(s)\\\\\n","&= \\sum_{s}\\eta(s)\\phi(s)\\\\\n","&= (\\sum_{s}\\eta(s))\\sum_{s}\\frac{\\eta(s)}{\\sum_{s}\\eta(s)}\\phi(s)\\\\\n","&\\propto \\sum_{s}\\frac{\\eta(s)}{\\sum_{s}\\eta(s)}\\phi(s)\\\\\n","&= \\sum_{s}d^{\\pi}(s)\\sum_{a}Q^{\\pi}(s,a)\\nabla_{\\theta}\\pi_{\\theta}(a|s)\n","\\end{align}\n","$$\n","证明完毕！"]},{"cell_type":"markdown","metadata":{"id":"D474406781854A6DBB3B7B4168AFC075","jupyter":{},"notebookId":"5f8f327c46ba5e00307827a4","slideshow":{"slide_type":"slide"},"tags":[]},"source":["# 第二部分：PPO算法\n","\n","之前介绍的TRPO在很多场景上都很成功，但是我们也发现了它的计算过程非常的复杂，每步更新的运算量非常大。于是，在2017年TRPO的改进版PPO算法被提出，它基于TRPO的思想，但是实现算法更加简单，避免了复杂的求KL散度的Hessian矩阵。并且大量的实验结果表明，PPO能够比TRPO学习的更快，这使得PPO一下子成为了非常流行的强化学习算法。如果我们想要尝试在一个新的环境用强化学习，那么PPO就属于那种可以首先尝试的算法。\n","\n","## PPO算法\n","\n","我们回忆一下TRPO的优化目标：\n","$$\n"," \\mathbb{E}_{s\\sim d^{\\pi_{\\theta_k}}}\\mathbb E_{a\\sim \\pi_{\\theta_k}(\\cdot|s)}\\left[ \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}A^{\\pi_{\\theta_k}}(s,a)\\right]\\quad\\text{s.t.}\\quad D_{KL}^{d^{\\pi_{\\theta_k}}}(\\pi_{\\theta_k},\\pi_\\theta)\\le\\delta\n","$$\n","\n","TRPO的做法是用KKT条件、近似、共轭梯度等方法直接对它求解的。PPO的优化目标同样是它，但PPO用了一些相对简单的方法来求解，其效果不差于TRPO。具体来说，PPO有两种形式，一是PPO-Penalty，二是PPO-Clip。\n","\n","### PPO-Penalty\n","\n","PPO-Penalty直接将KL散度的限制放进了目标函数中，这就变成了一个无约束的优化问题，然后在迭代的过程中不断更新KL散度前的系数。即\n","$$\n","\\theta=\\mathop{\\arg\\max}_\\theta \\mathbb{E}_{s\\sim d^{\\pi_{\\theta_k}}}\\mathbb E_{a\\sim \\pi_{\\theta_k}(\\cdot|s)}\\left[ \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}A^{\\pi_{\\theta_k}}(s,a)-\\beta D_{KL}[\\pi_{\\theta_k}(\\cdot|s),\\pi_\\theta(\\cdot|s)]\\right]\n","$$\n","\n","令 $d=D_{KL}^{d^{\\pi_{\\theta_k}}}(\\pi_{\\theta_k},\\pi_\\theta)$，$\\beta$ 的更新规则如下：\n","1. 如果$d<d_{target}/1.5$，那么$\\beta\\leftarrow \\beta/2$\n","1. 如果$d>d_{target}\\times1.5$，那么$\\beta\\leftarrow \\beta\\times 2$\n","\n","### PPO-Clip\n","\n","PPO-Clip更加直接，其直接在目标函数里进行限制，以保证新的参数和旧的参数的差距不会太大，即\n","\n","$$\n","\\theta=\\mathop{\\arg\\max}_\\theta \\mathbb{E}_{s\\sim d^{\\pi_{\\theta_k}}}\\mathbb E_{a\\sim \\pi_{\\theta_k}(\\cdot|s)}\\left[\\min \\left(\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)} A^{\\pi_{\\theta_k}}(s,a),\\text{clip}\\left(\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)},1-\\epsilon,1+\\epsilon\\right)A^{\\pi_{\\theta_k}}(s,a)\\right)\\right]\n","$$\n","\n","其中 $\\text{clip}(x,l,r):=\\max(\\min(x,r),l)$ ，即把 $x$ 限制在 $[l,r]$ 内。上式中$\\epsilon$是一个超参数，表示clip的范围。如果$A(s,a)>0$，说明这个动作的Q值高于平均，最大化这个式子会增大$\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}$，但不会让其超过$1+\\epsilon$。反之，如果$A(s,a)<0$，最大化这个式子会减小$\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}$，但不会让其超过$1-\\epsilon$。\n"]},{"cell_type":"markdown","metadata":{},"source":["## PPO代码实践\n","\n","和TRPO一样，我们仍然在两个环境CartPle和Pendulum上测试PPO算法。实验表明，PPO-Clip总是比PPO-Penalty表现得更好。因此下面我们只出PPO-Clip的代码。\n","### 环境1: CartPole\n","我们先在离散动作环境CartPole上测试算法效果，下面实现具体的PPO算法。注意：只需在ActorCritic算法基础上修改loss计算的update函数即可。"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ========================================\n","# 【代码实现 3】: 补全完成PPO算法（具体实现与ActorCritic类似）\n","# ========================================\n","class PPO(RLAlgo):\n","\n","    def __init__(\n","        self, \n","        clip_range=0.2,\n","        **kwargs,\n","    ):\n","        super(PPO, self).__init__( **kwargs)\n","        self._clip_range = clip_range\n","    \n","    def update(self, batch, gamma):\n","        s = batch[\"state\"]\n","        a = batch[\"action\"]\n","        r = batch[\"reward\"].reshape(-1, 1)\n","        s1 = batch[\"next_state\"]\n","        adv = batch[\"adv\"]\n","        done = batch[\"done\"].reshape(-1, 1)\n","        old_log_prob = batch[\"log_prob\"].reshape(-1, 1)\n","        \n","        pass\n","        \n","        self.soft_update(self.value, self.target_value, self._tau)"]},{"cell_type":"markdown","metadata":{},"source":["接下来，开始训练PPO算法"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 请自行调节参数\n","learning_rate = \n","hidden_size= \n","agent = PPO(\n","        hidden_size=hidden_size, \n","        state_space=env.observation_space, \n","        action_space=env.action_space, \n","        learning_rate=learning_rate,\n","    )\n","ppo_cartpole_infos = train(agent, 2000, 10)"]},{"cell_type":"markdown","metadata":{},"source":["**请分别测试ActorCritic和PPO算法在该环境下的效果。**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.ylabel(\"Reward\")\n","plt.xlabel(\"Frame\")\n","infos = [ac_cartpole_infos, ppo_cartpole_infos]\n","labels = [\"ac\", \"ppo\"]\n","for info in infos:\n","    x, y = info[\"episodes\"],info[\"rewards\"]\n","    y, x = moving_average(y, x)\n","    plt.plot(x, y)\n","plt.legend(labels)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### 环境2：Pendulum\n","\n","接下来，我们比较两个算法在连续环境下的效果。对于不同环境，我们只需修改policy输出的分布即可，相关代码在RLAlgo的dist函数。"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["env_name = \"Pendulum-v0\"\n","env = gym.make(env_name)\n","\n","seed = 0\n","env.seed(seed)\n","np.random.seed(seed)\n","# torch.manual_seed(seed)\n"]},{"cell_type":"markdown","metadata":{},"source":["**请分别测试ActorCritic和PPO算法在该环境下的效果。**\n","\n","（由于ActorCritic在此环境下效果较差，不需要过度调参。只需保证PPO算法最后结果在$-500$以上即可。）"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 请自行调节参数\n","hidden_size=\n","learning_rate = \n","agent = AdvantageActorCritic(\n","        hidden_size, \n","        env.observation_space, \n","        env.action_space, \n","        learning_rate, \n","        discrete_action=False\n","    )\n","\n","ac_pendulum_infos = train(agent, 1500, 1, discrete_action=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 请自行调节参数\n","hidden_size=\n","learning_rate=\n","agent = PPO(\n","        hidden_size=hidden_size, \n","        state_space=env.observation_space, \n","        action_space=env.action_space, \n","        learning_rate=learning_rate,\n","        discrete_action=False,\n","    )\n","ppo_pendulum_infos = train(agent, 2500, 10, discrete_action=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.ylabel(\"Reward\")\n","plt.xlabel(\"Frame\")\n","infos = [ac_pendulum_infos, ppo_pendulum_infos]\n","labels = [\"ac\", \"ppo\"]\n","for info in infos:\n","    x, y = info[\"episodes\"],info[\"rewards\"]\n","    y, x = moving_average(y, x)\n","    plt.plot(x, y)\n","plt.legend(labels)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.5.2","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":4}