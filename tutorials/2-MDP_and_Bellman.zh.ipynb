{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 马尔可夫过程与贝尔曼方程"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman equation\n",
    "\n",
    "### **1. 确定性过程下的 Bellman equation**\n",
    "\n",
    "设 $t$ 时刻state记为 $s_t$，action记为 $a_t$。考虑如下最大化收益的决策问题：在 $s_t$ 状态下 agent 采取 $a_t$ 的action时会得到 $R(s_t,a_t)$ 的奖励，同时会以确定的状态转移函数转移到下一状态，即 $s_{t+1} = T(s_t, a_t)$。考虑衰减因子为 $\\lambda$，则可以定义如下的 **value function**\n",
    "\n",
    "$$ V\\left(s_t\\right)=\\max _{\\left\\{a_{k}\\right\\}_{k=t}^{\\infty}} \\sum_{k=t}^{\\infty} \\lambda^{k-t} R\\left(s_{t}, a_{t}\\right) $$\n",
    "\n",
    "采取一系列合适的 action 最大化该 value function 即取得最大收益。$V(s_t)$ 可展开写成\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "V(s_t) & = \\max _{a_{t}}\\left\\{R\\left(s_{t}, a_{t}\\right)+\\lambda\\left[\\max _{\\left\\{a_{k}\\right\\}_{k=t+1}^{\\infty}} \\sum_{k=t+1}^{\\infty} \\lambda^{k-(t+1)} R\\left(s_{t}, a_{t}\\right)\\right]\\right\\} \\\\\n",
    "& = \\max _{a_{t}}\\left\\{ R\\left(s_{t}, a_{t}\\right) + \\lambda V(s_{t+1}) \\right\\} \\\\\n",
    "& = \\max _{a_{t}}\\left\\{ R\\left(s_{t}, a_{t}\\right) + \\lambda V\\left(T\\left(s_t, a_t\\right)\\right) \\right\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "这就是在奖励、状态转移方式均以确定的函数形式给出，且两函数均只与当前 state 和 action 有关时的 **Bellman equation**.\n",
    "\n",
    "注意到 Bellman equation 就是把总收益写成了递归的形式，因此求解 Bellman equation 可以得到\n",
    "1. value function $V$ 的具体形式\n",
    "2. 一系列最优的action，或者说在给定state $s_t$下的最优 $a_t$，即 state 到 action 的映射，记作 $\\pi (\\cdot)$，叫做 **policy function**. \n",
    "\n",
    "### **2. MDP 下的 Bellman equation: 随机状态转移**\n",
    "\n",
    "考虑状态转移形式不再以确定函数 $T$ 给出，而是在 $s_t$ 的状态下采取 $a_t$ 会依概率转移到一个随机的状态，这个时候状态是一个随机变量，以大写记作 $S_t$，假设它具有马尔可夫性。经过映射 $\\pi(\\cdot)$ 后 action 也是一个随机变量，记作 $A_t$。这个时候用条件概率描述状态转移，即\n",
    "\n",
    "$$ P_{\\pi}(s, s^{\\prime}) \\doteq \\operatorname{Pr}\\left\\{S_{t+1}=s^{\\prime} \\mid S_t=s, A_t=a=\\pi(s) \\right\\} $$\n",
    "\n",
    "其中 $Pr\\{\\cdot\\}$ 表示与 $\\{\\}$ 内随机变量和条件对应的概率分布函数。这里假设所有随机变量都在有限集合内取值，如 $ s^{\\prime} \\in \\mathcal{S}$, $a \\in \\mathcal{A}(s)$. \n",
    "\n",
    "注意到奖励 $R(S_t, A_t)$ 也是一个随机变量，记作 $R_{t+1}$，此时 value function 需要用期望的形式重写。为方便引入 return $G_t$ 定义为\n",
    "$$\n",
    "G_t \\doteq \\sum_{k=0}^{\\infty} \\lambda^{k} R_{t+k+1}\n",
    "$$\n",
    "\n",
    "在给定策略 $\\pi$ 下的 value function 写为\n",
    "$$ V(s) \\doteq V_{\\pi}(s) = \\mathbb{E}\\left[G_t \\mid S_t = s\\right] $$\n",
    "\n",
    "同样尝试把它写成递归的形式，从而得到 Bellman equation：\n",
    "\n",
    "$$ V(s) = \\mathbb{E}\\left[R_{t+1} \\mid S_t = s\\right] + \\lambda \\mathbb{E}\\left[G_{t+1} \\mid S_t = s\\right] $$\n",
    "\n",
    "注意到 $\\pi (\\cdot)$ 为确定函数，则上式第二项\n",
    "$$ \\begin{aligned}\n",
    "\\mathbb{E}\\left[G_{t+1} \\mid S_t = s\\right] \n",
    "& = \\sum_{g \\in \\mathcal{G}} g \\operatorname{Pr}\\left\\{G_{t+1}=g \\mid s\\right\\} \\\\\n",
    "& = \\sum_{g \\in \\mathcal{G}} g \\operatorname{Pr}\\left\\{G_{t+1}=g \\mid s, a=\\pi(s)\\right\\} \\\\\n",
    "& = \\sum_{g \\in \\mathcal{G}} \\sum_{s^{\\prime} \\in \\mathcal{S}} g \\operatorname{Pr}\\left\\{g \\mid s, a, S_{t+1}=s^{\\prime}\\right\\} \\operatorname{Pr}\\left\\{S_{t+1}=s^{\\prime} \\mid s, a\\right\\} \\\\\n",
    "& = \\sum_{s^{\\prime} \\in \\mathcal{S}} \\operatorname{Pr}\\left\\{s^{\\prime} \\mid s, a\\right\\} \\sum_{g \\in \\mathcal{G}} g \\operatorname{Pr}\\left\\{g \\mid s, a, s^{\\prime}\\right\\} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中第二到第三个等式由全概率公式给出。由马尔可夫假设和 $G_{t+1}$ 的定义，有 $ \\operatorname{Pr}\\left\\{g \\mid s, a, s^{\\prime}\\right\\} = \\operatorname{Pr}\\left\\{g \\mid s^{\\prime}\\right\\}$，即\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "\\mathbb{E}\\left[G_{t+1} \\mid S_t = s\\right] \n",
    "& = \\sum_{s^{\\prime} \\in \\mathcal{S}} \\operatorname{Pr}\\left\\{s^{\\prime} \\mid s, a\\right\\} \\sum_{g \\in \\mathcal{G}} g \\operatorname{Pr}\\left\\{g \\mid s^{\\prime}\\right\\} \\\\\n",
    "& = \\sum_{s^{\\prime} \\in \\mathcal{S}} \\operatorname{Pr}\\left\\{s^{\\prime} \\mid s, a\\right\\} \\mathbb{E}\\left[G_{t+1} \\mid S_{t+1} = s^{\\prime}\\right] \\\\\n",
    "& \\doteq \\sum_{s^{\\prime} \\in \\mathcal{S}} P_\\pi(s, s^{\\prime}) V(s^{\\prime})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "记 $R(s) = \\mathbb{E}\\left[R_{t+1} \\mid S_t = s\\right]$，则有常见的递归形式\n",
    "\n",
    "$$\n",
    "V(s) = R(s) + \\lambda \\sum_{s^{\\prime} \\in \\mathcal{S}} P_\\pi(s, s^{\\prime}) V(s^{\\prime})\n",
    "$$\n",
    "\n",
    "对 $\\pi(\\cdot)$ 优化即为 Bellman equation. \n",
    "\n",
    "\n",
    "### **3. MDP 下的 Bellman equation：随机奖励和决策**\n",
    "\n",
    "进一步地，考虑奖励也只依概率给出；并且决策也不再是确定的，而是依概率决策。此时 policy function 变为概率分布函数，即\n",
    "\n",
    "$$ \\pi(a \\mid s) \\doteq \\operatorname{Pr} \\{A_t=a \\mid S_t=s\\} $$\n",
    "\n",
    "agent 和 environment 交互的过程可以表述为这么一串随机过程：\n",
    "\n",
    "$$ S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, ... $$\n",
    "\n",
    "这个时候用条件概率描述 “agent 在特定的 state 下采取 action，获得奖励并转移到下一个状态” 这整件事：\n",
    "\n",
    "$$ p\\left(s^{\\prime}, r \\mid s, a\\right) \\doteq \\operatorname{Pr}\\left\\{S_{t+1}=s^{\\prime}, R_{t+1}=r \\mid S_{t}=s, A_{t}=a\\right\\} $$\n",
    "\n",
    "\n",
    "这个时候再考虑 value function，同样以 $ V(s) = \\mathbb{E}\\left[G_t \\mid S_t = s\\right] $ 形式给出，尝试推导 Bellman equation.\n",
    "\n",
    "首先有\n",
    "$$ V(s) = \\mathbb{E}\\left[R_{t+1} \\mid S_t = s\\right] + \\lambda \\mathbb{E}\\left[G_{t+1} \\mid S_t = s\\right]\n",
    "$$\n",
    "\n",
    "不断地把边缘概率展开，并反复利用全概率公式，有第一项\n",
    "$$ \\begin{aligned}\n",
    "\\mathbb{E}\\left[R_{t+1} \\mid s\\right] & =\\sum_{r \\in \\mathcal{R}} r \\operatorname{Pr}\\{r \\mid s\\} \\\\\n",
    "& =\\sum_{r \\in \\mathcal{R}} \\sum_{a \\in \\mathcal{A}(s)} r \\operatorname{Pr}\\left\\{r \\mid s, A_{t}=a\\right\\} \\operatorname{Pr}\\left\\{A_{t}=a \\mid s\\right\\} \\\\\n",
    "& =\\sum_{r \\in \\mathcal{R}} \\sum_{a \\in \\mathcal{A}(s)} r \\operatorname{Pr}\\{r \\mid s, a\\} \\pi(a \\mid s) \\\\\n",
    "& =\\sum_{a \\in \\mathcal{A}(s)} \\pi(a \\mid s) \\sum_{r \\in \\mathcal{R}} r \\operatorname{Pr}\\{r \\mid s, a\\} \\\\\n",
    "& =\\sum_{a \\in \\mathcal{A}(s)} \\pi(a \\mid s) \\sum_{r \\in \\mathcal{R}} r \\sum_{s^{\\prime} \\in \\mathcal{S}} \\operatorname{Pr}\\left\\{r, S_{t+1}=s^{\\prime} \\mid s, a\\right\\} \\\\\n",
    "& =\\sum_{a \\in \\mathcal{A}(s)} \\pi(a \\mid s) \\sum_{r \\in \\mathcal{R}} \\sum_{s^{\\prime} \\in \\mathcal{S}} p\\left(r, s^{\\prime} \\mid s, a\\right) r\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "第二项\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{E}\\left[G_{t+1} \\mid s\\right] & =\\sum_{g \\in \\mathcal{G}} g \\operatorname{Pr}\\left\\{G_{t+1}=g \\mid s\\right\\} \\\\\n",
    "& =\\sum_{g \\in \\mathcal{G}} \\sum_{a \\in \\mathcal{A}(s)} g \\operatorname{Pr}\\left\\{g \\mid s, A_{t}=a\\right\\} \\operatorname{Pr}\\left\\{A_{t}=a \\mid s\\right\\} \\\\\n",
    "& =\\sum_{a \\in \\mathcal{A}(s)} \\pi(a \\mid s) \\sum_{g \\in \\mathcal{G}} g \\operatorname{Pr}\\{g \\mid s, a\\} \\\\\n",
    "& =\\sum_{a \\in \\mathcal{A}(s)} \\pi(a \\mid s) \\sum_{g \\in \\mathcal{G}} \\sum_{s^{\\prime} \\in \\mathcal{S}} g \\operatorname{Pr}\\left\\{g \\mid s, a, S_{t+1}=s^{\\prime}\\right\\} \\operatorname{Pr}\\left\\{S_{t+1}=s^{\\prime} \\mid s, a\\right\\} \\\\\n",
    "& =\\sum_{a \\in \\mathcal{A}(s)} \\pi(a \\mid s) \\sum_{s^{\\prime} \\in \\mathcal{S}} \\operatorname{Pr}\\left\\{s^{\\prime} \\mid s, a\\right\\} \\sum_{g \\in \\mathcal{G}} g \\operatorname{Pr}\\left\\{g \\mid s, a, s^{\\prime}\\right\\} \\\\\n",
    "& =\\sum_{a \\in \\mathcal{A}(s)} \\pi(a \\mid s) \\sum_{r \\in \\mathcal{R}} \\sum_{s^{\\prime} \\in \\mathcal{S}} p\\left(r, s^{\\prime} \\mid s, a\\right) \\sum_{g \\in \\mathcal{G}} g \\operatorname{Pr}\\left\\{g \\mid s, a, s^{\\prime}\\right\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "由马尔可夫性有 $\\operatorname{Pr}\\left\\{g \\mid s, a, s^{\\prime}\\right\\} = \\operatorname{Pr}\\left\\{g \\mid s^{\\prime}\\right\\}$，所以\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{E}\\left[G_{t+1} \\mid s\\right]\n",
    "& = \\sum_{a \\in \\mathcal{A}(s)} \\pi(a \\mid s) \\sum_{r \\in \\mathcal{R}} \\sum_{s^{\\prime} \\in \\mathcal{S}} p\\left(r, s^{\\prime} \\mid s, a\\right) \\sum_{g \\in \\mathcal{G}} g \\operatorname{Pr}\\left\\{g \\mid s^{\\prime}\\right\\} \\\\\n",
    "& = \\sum_{a \\in \\mathcal{A}(s)} \\pi(a \\mid s) \\sum_{r \\in \\mathcal{R}} \\sum_{s^{\\prime} \\in \\mathcal{S}} p\\left(r, s^{\\prime} \\mid s, a\\right) \\mathbb{E}\\left[G_{t+1} \\mid s^{\\prime}\\right]  \\\\\n",
    "& = \\sum_{a \\in \\mathcal{A}(s)} \\pi(a \\mid s) \\sum_{r \\in \\mathcal{R}} \\sum_{s^{\\prime} \\in \\mathcal{S}} p\\left(r, s^{\\prime} \\mid s, a\\right) V(s^{\\prime})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "从而可以得到一种常见递归形式\n",
    "\n",
    "$$ V(s) = \\sum_{a \\in \\mathcal{A}(s)} \\pi(a \\mid s) \\sum_{r \\in \\mathcal{R}} \\sum_{s^{\\prime} \\in \\mathcal{S}} p\\left(r, s^{\\prime} \\mid s, a\\right) \\left[r + \\lambda V(s^{\\prime})\\right]\n",
    "$$\n",
    "\n",
    "或者，分别用记号表示状态转移概率 $P_a(s,s^{\\prime})$ 和转移后的期望奖励 $R_a(s,s^{\\prime})$：\n",
    "\n",
    "$$\n",
    "P_a(s,s^{\\prime}) \\doteq \\operatorname{Pr}\\left\\{S_{t+1}=s^{\\prime} \\mid S_{t}=s, A_{t}=a\\right\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "R_a(s,s^{\\prime}) \\doteq \\mathbb{E}\\left[R_{t+1} \\mid S_{t}=s, A_{t}=a, S_{t+1}=s^{\\prime}\\right]\n",
    "$$\n",
    "\n",
    "则由上述的推导，第一项可化为\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "\\mathbb{E}\\left[R_{t+1} \\mid s\\right]\n",
    "& = \\sum_{a \\in \\mathcal{A}(s)} \\pi(a \\mid s) \\sum_{r \\in \\mathcal{R}} r \\operatorname{Pr}\\{r \\mid s, a\\} \\\\\n",
    "& = \\sum_{a \\in \\mathcal{A}(s)} \\pi(a \\mid s) \\sum_{r \\in \\mathcal{R}} \\sum_{s^{\\prime} \\in \\mathcal{S}} r \\operatorname{Pr}\\left\\{r \\mid s, a, S_{t+1}=s^{\\prime}\\right\\} \\operatorname{Pr}\\left\\{S_{t+1}=s^{\\prime} \\mid s, a\\right\\} \\\\\n",
    "& = \\sum_{a \\in \\mathcal{A}(s)} \\pi(a \\mid s) \\sum_{s^{\\prime} \\in \\mathcal{S}} \\operatorname{Pr}\\left\\{s^{\\prime} \\mid s, a\\right\\} \\sum_{r \\in \\mathcal{R}}  r \\operatorname{Pr}\\left\\{r \\mid s, a, s^{\\prime}\\right\\} \\\\\n",
    "& = \\sum_{a \\in \\mathcal{A}(s)} \\pi(a \\mid s) \\sum_{s^{\\prime} \\in \\mathcal{S}} P_a(s,s^{\\prime}) R_a(s,s^{\\prime})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "同理第二项可化为\n",
    "\n",
    "$$ \\mathbb{E}\\left[G_{t+1} \\mid s\\right] = \\sum_{a \\in \\mathcal{A}(s)} \\pi(a \\mid s) \\sum_{s^{\\prime} \\in \\mathcal{S}} P_a(s,s^{\\prime}) V(s^{\\prime})\n",
    "$$\n",
    "\n",
    "则有另一种常见递归形式\n",
    "$$ V(s) = \\sum_{a \\in \\mathcal{A}(s)} \\pi(a \\mid s) \\sum_{s^{\\prime} \\in \\mathcal{S}} P_a(s,s^{\\prime}) \\left[R_a(s,s^{\\prime}) + \\lambda V(s^{\\prime})\\right]\n",
    "$$\n",
    "\n",
    "这个时候可以引出 Q-function，即 state 和 action 都给定时总收益的期望: \n",
    "$$ \\begin{aligned}\n",
    "Q(s,a) & \\doteq \\mathbb{E}\\left[G_t \\mid S_t = s, A_t = a\\right] \\\\\n",
    "& = \\sum_{s^{\\prime} \\in \\mathcal{S}} P_a(s,s^{\\prime}) \\left[R_a(s,s^{\\prime}) + \\lambda V(s^{\\prime})\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "可以得出 value function 和 Q-funciton 间的关系：\n",
    "$$ V(s) = \\sum_{a \\in \\mathcal{A}(s)} \\pi(a \\mid s) Q(s,a) $$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
